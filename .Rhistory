# Get month tag
month <- regmatches(dateTimeText, gregexpr("[a-zA-Z]{3}", dateTimeText))
monthTags <- c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")
monthNrs <- c("01", "02", "03", "04", "05", "06", "07", "08", "09", "10", "11", "12")
for(m in 1:12) { month <- sapply(1:length(month), function(x) gsub(monthTags[m], monthNrs[m], month[x])) }
dateTimeText <- sapply(1:length(dateTimeText), function(x) gsub(month[x], "", dateTimeText[x]))
dateTimeText
month <- regmatches(dateTimeText, gregexpr("[a-zA-Z]{3}", dateTimeText))
dateTimeText <- sapply(1:length(dateTimeText), function(x) gsub(month[x], "", dateTimeText[x]))
monthTags <- c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")
monthNrs <- c("01", "02", "03", "04", "05", "06", "07", "08", "09", "10", "11", "12")
for(m in 1:12) { month <- sapply(1:length(month), function(x) gsub(monthTags[m], monthNrs[m], month[x])) }
dateTimeText
# Date
date <- regmatches(dateTimeText, gregexpr(" [0-9]{1,2} ", dateTimeText))
date <- gsub(" ", "", date)
dateTimeText <- sapply(1:length(dateTimeText), function(x) gsub(date[x], "", dateTimeText[x]))
date
dateTimeText <- aData[,"timeStamp"][96]
dateTimeText
dateTimeText <- "Sun, 02 Aug 2015 17:11:35 GMT"
# Get year
year <- unlist(regmatches(dateTimeText, gregexpr("201[0-9]", dateTimeText)))
dateTimeText <- sapply(1:length(dateTimeText), function(x) gsub(year[x], "", dateTimeText[x]))
# Get weekday
daysVec <- c("mon|tue|wed|thu|fri|sat|sun|Mon|Tue|Wed|Thu|Fri|Sat|Sun")
weekDay <- unlist(regmatches(dateTimeText, gregexpr(daysVec, dateTimeText)))
dateTimeText <- sapply(1:length(dateTimeText), function(x) gsub(weekDay[x], "", dateTimeText[x]))
# Get time zone
timeZone <- unlist(regmatches(dateTimeText, gregexpr("[A-Z]{3}|-[0-9]{2}00", dateTimeText)))
dateTimeText <- sapply(1:length(dateTimeText), function(x) gsub(timeZone[x], "", dateTimeText[x]))
# Get time section
timeText <- unlist(regmatches(dateTimeText, gregexpr("[0-2][0-9]:[0-5][0-9]:[0-5][0-9]", dateTimeText)))
dateTimeText <- sapply(1:length(dateTimeText), function(x) gsub(timeText[x], "", dateTimeText[x]))
# Get month tag
month <- regmatches(dateTimeText, gregexpr("[a-zA-Z]{3}", dateTimeText))
dateTimeText <- sapply(1:length(dateTimeText), function(x) gsub(month[x], "", dateTimeText[x]))
monthTags <- c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")
monthNrs <- c("01", "02", "03", "04", "05", "06", "07", "08", "09", "10", "11", "12")
for(m in 1:12) { month <- sapply(1:length(month), function(x) gsub(monthTags[m], monthNrs[m], month[x])) }
dateTimeText
regmatches(dateTimeText, gregexpr(" [0-9]{1,2} ", dateTimeText))
dateTimeText <- aData[,"timeStamp"][96]
# Get year
year <- unlist(regmatches(dateTimeText, gregexpr("201[0-9]", dateTimeText)))
dateTimeText <- sapply(1:length(dateTimeText), function(x) gsub(year[x], "", dateTimeText[x]))
# Get weekday
daysVec <- c("mon|tue|wed|thu|fri|sat|sun|Mon|Tue|Wed|Thu|Fri|Sat|Sun")
weekDay <- unlist(regmatches(dateTimeText, gregexpr(daysVec, dateTimeText)))
dateTimeText <- sapply(1:length(dateTimeText), function(x) gsub(weekDay[x], "", dateTimeText[x]))
# Get time zone
timeZone <- unlist(regmatches(dateTimeText, gregexpr("[A-Z]{3}|-[0-9]{2}00", dateTimeText)))
dateTimeText <- sapply(1:length(dateTimeText), function(x) gsub(timeZone[x], "", dateTimeText[x]))
# Get time section
timeText <- unlist(regmatches(dateTimeText, gregexpr("[0-2][0-9]:[0-5][0-9]:[0-5][0-9]", dateTimeText)))
dateTimeText <- sapply(1:length(dateTimeText), function(x) gsub(timeText[x], "", dateTimeText[x]))
# Get month tag
month <- regmatches(dateTimeText, gregexpr("[a-zA-Z]{3}", dateTimeText))
dateTimeText <- sapply(1:length(dateTimeText), function(x) gsub(month[x], "", dateTimeText[x]))
monthTags <- c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")
monthNrs <- c("01", "02", "03", "04", "05", "06", "07", "08", "09", "10", "11", "12")
for(m in 1:12) { month <- sapply(1:length(month), function(x) gsub(monthTags[m], monthNrs[m], month[x])) }
# Date
date <- regmatches(dateTimeText, gregexpr(" [0-9]{1,2} ", dateTimeText))
date <- gsub(" ", "", date)
dateTimeText <- sapply(1:length(dateTimeText), function(x) gsub(date[x], "", dateTimeText[x]))
# Compile date string
dateTimeText <- paste(year,"-",month,"-",date, " ", timeText, sep="")
dateTimeText
date
if(length(date==1)) {date <- paste("0", date, sep="")}
date
date <- c("31", "2")
if(length(date==1)) {date <- paste("0", date, sep="")}
date
date <- c("31", "2")
date
date[length(date)==1] <- paste("0", date[length(date)==1], sep="")
date
date[length(date)==1]
length(date)
library(stringr)
dateTimeText <- aData[,"timeStamp"][96]
# Get year
year <- unlist(regmatches(dateTimeText, gregexpr("201[0-9]", dateTimeText)))
dateTimeText <- sapply(1:length(dateTimeText), function(x) gsub(year[x], "", dateTimeText[x]))
# Get weekday
daysVec <- c("mon|tue|wed|thu|fri|sat|sun|Mon|Tue|Wed|Thu|Fri|Sat|Sun")
weekDay <- unlist(regmatches(dateTimeText, gregexpr(daysVec, dateTimeText)))
dateTimeText <- sapply(1:length(dateTimeText), function(x) gsub(weekDay[x], "", dateTimeText[x]))
# Get time zone
timeZone <- unlist(regmatches(dateTimeText, gregexpr("[A-Z]{3}|-[0-9]{2}00", dateTimeText)))
dateTimeText <- sapply(1:length(dateTimeText), function(x) gsub(timeZone[x], "", dateTimeText[x]))
# Get time section
timeText <- unlist(regmatches(dateTimeText, gregexpr("[0-2][0-9]:[0-5][0-9]:[0-5][0-9]", dateTimeText)))
dateTimeText <- sapply(1:length(dateTimeText), function(x) gsub(timeText[x], "", dateTimeText[x]))
# Get month tag
month <- regmatches(dateTimeText, gregexpr("[a-zA-Z]{3}", dateTimeText))
dateTimeText <- sapply(1:length(dateTimeText), function(x) gsub(month[x], "", dateTimeText[x]))
monthTags <- c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")
monthNrs <- c("01", "02", "03", "04", "05", "06", "07", "08", "09", "10", "11", "12")
for(m in 1:12) { month <- sapply(1:length(month), function(x) gsub(monthTags[m], monthNrs[m], month[x])) }
# Date
date <- regmatches(dateTimeText, gregexpr(" [0-9]{1,2} ", dateTimeText))
date <- gsub(" ", "", date)
dateTimeText <- sapply(1:length(dateTimeText), function(x) gsub(date[x], "", dateTimeText[x]))
# Compile date string
dateTimeText <- paste(year,"-",month,"-",date, " ", timeText, sep="")
# Apply correction
timeZoneText <- timeZone
timeZoneCorrection <- as.numeric(regmatches(timeZone, gregexpr("-[0-9]{2}00", timeZone)))/100
timeZoneCorrection[is.na(timeZoneCorrection)] <- 0
timeZoneText[timeZoneCorrection!=0] <- "UTC"
timeZoneText <- gsub("EDT", "America/New_York", timeZoneText)
dateTimeText <- ymd_hms(dateTimeText) -  hours(timeZoneCorrection)
# Overwrite incorrectly assumed timezones
dateTimeTextTZ <- rep(NA,length(dateTimeText))
for(x in 1:length(dateTimeText)) {
temp <- force_tz(dateTimeText[x], timeZoneText[x])
dateTimeTextTZ[x] <- format(temp, tz="America/New_York",usetz=TRUE)
}
dateTimeTextTZ
dateTimeText <- c("Sat, 01 Aug 2015 14:59:01 -0400", "Sat, 01 Aug 2015 18:45:03 GMT", "Sat, 01 Aug 2015 14:01:46 EDT", "Sun, 02 Aug 2015 17:11:35 GMT")
GetTime(dateTimeText)
# Load packages
library(XML)
library(lubridate)
if(Sys.info()["nodename"]!="MA2") {
source("/home/finance/Gitfin/dbFunctions.R")
source("/home/finance/Gitfin/SourceTimeFunction.R")
} else {
source("SourceTimeFunction.R")
}
# Function to get data from url
GetTop <- function(url) {
con = url(url); sourceCode = readLines(con); close(con)
xmlfile <- xmlTreeParse(sourceCode)
xmltop = xmlRoot(xmlfile)
return(xmltop)
}
# Function to extract data from node
GetNodeDetails <- function(node, timeStampTag, titleTag, descTag, linkTag, getTicker) {
# Process data
nData <- xmlSApply(node, function(x) xmlSApply(x, xmlValue))
names(nData) <- gsub(".text", "", names(nData))
nData <- unlist(nData)
# Get elements
timeStamp <- nData[timeStampTag]
title <- nData[titleTag]
if(!is.na(descTag)) { description <- nData[descTag] } else { description <- NA }
if(!is.na(linkTag)) { link <- nData[linkTag] } else { link <- NA }
# Tickers
if(getTicker) {
tData <- as.data.frame(nData, stringsAsFactors=FALSE)
tickers <- grep("^([A-Z]{1,5})$", tData[,1], value=TRUE)
tickers <- paste(unique(tickers),collapse=",")
} else {
tickers <- NA
}
# Return data
return(data.frame(timeStamp, title, description, link, tickers, row.names = NULL))
}
# Function to pull data and format it
ProcessTop <- function(siteName, xmltop, xpath, timestampTag, titleTag, descTag, linkTag, getTicker) {
nodes <- getNodeSet(xmltop, xpath)
nodeData <- lapply(nodes, GetNodeDetails, timestampTag, titleTag, descTag, linkTag, getTicker)
returnData <- as.data.frame(do.call(rbind, nodeData), stringsAsFactors = FAlSE)
returnData <- cbind(siteName, returnData)
return(returnData)
}
#################
### PULL DATA ###
#################
# Create space to save data
aData <- c()
# SEEKING ALPHA
xmltop <- GetTop("http://seekingalpha.com/feed.xml")
newsData <- ProcessTop("seekingalpha.com", xmltop, "//channel/item", "pubDate", "title", NA, "link", TRUE)
aData <- rbind(aData, newsData)
# Motley Fool
xmltop <- GetTop("http://www.fool.com/feeds/index.aspx?id=foolwatch&format=rss2")
newsData <- ProcessTop("fool.com", xmltop, "//channel/item", "pubDate", "title", "description", "guid", TRUE)
aData <- rbind(aData, newsData)
# Wall Street Journal
xmltop <- GetTop("http://www.wsj.com/xml/rss/3_7031.xml")
newsData <- ProcessTop("wsj.com", xmltop, "//rss/channel/item", "pubDate", "title", "description", "link", FALSE)
aData <- rbind(aData, newsData)
# Financial Times
xmltop <- GetTop("http://www.ft.com/rss/markets")
newsData <- ProcessTop("ft.com", xmltop, "//rss/channel/item", "pubDate", "title", "description", "guid", FALSE)
aData <- rbind(aData, newsData)
# Forbes
xmltop <- GetTop("http://www.forbes.com/markets/index.xml")
newsData <- ProcessTop("forbes.com", xmltop, "//rss/channel/item", "pubDate", "title", "description", "link", FALSE)
aData <- rbind(aData, newsData)
# Show data collected
head(aData); dim(aData)
aData[,"title"] <- gsub("'", "", aData[,"title"])
aData[,"description"] <- gsub("'", "", aData[,"description"])
aData[,"timeStamp"] <- GetTime(aData[,"timeStamp"])
aData <- aData[order(aData[,"timeStamp"], decreasing=TRUE), ]
print(head(aData))
print(dim(aData))
fix(aData)
# Load packages
library(XML)
library(lubridate)
if(Sys.info()["nodename"]!="MA2") {
source("/home/finance/GitFin/dbFunctions.R")
source("/home/finance/GitFin/SourceTimeFunction.R")
} else {
source("SourceTimeFunction.R")
}
#################################################
### FUNCTIONS FOR PULLING AND PROCESSING DATA ###
#################################################
# Function to get data from url
GetTop <- function(url) {
con = url(url); sourceCode = readLines(con); close(con)
xmlfile <- xmlTreeParse(sourceCode)
xmltop = xmlRoot(xmlfile)
return(xmltop)
}
# Function to extract data from node
GetNodeDetails <- function(node, timeStampTag, titleTag, descTag, linkTag, getTicker) {
# Process data
nData <- xmlSApply(node, function(x) xmlSApply(x, xmlValue))
names(nData) <- gsub(".text", "", names(nData))
nData <- unlist(nData)
# Get elements
timeStamp <- nData[timeStampTag]
title <- nData[titleTag]
if(!is.na(descTag)) { description <- nData[descTag] } else { description <- NA }
if(!is.na(linkTag)) { link <- nData[linkTag] } else { link <- NA }
# Tickers
if(getTicker) {
tData <- as.data.frame(nData, stringsAsFactors=FALSE)
tickers <- grep("^([A-Z]{1,5})$", tData[,1], value=TRUE)
tickers <- paste(unique(tickers),collapse=",")
} else {
tickers <- NA
}
# Return data
return(data.frame(timeStamp, title, description, link, tickers, row.names = NULL))
}
# Function to pull data and format it
ProcessTop <- function(siteName, xmltop, xpath, timeStampTag, titleTag, descTag, linkTag, getTicker) {
nodes <- getNodeSet(xmltop, xpath)
nodeData <- lapply(nodes, GetNodeDetails, timeStampTag, titleTag, descTag, linkTag, getTicker)
returnData <- as.data.frame(do.call(rbind, nodeData), stringsAsFactors = FAlSE)
returnData <- cbind(siteName, returnData)
return(returnData)
}
#################
### PULL DATA ###
#################
# Create space to save data
aData <- c()
xmltop <- GetTop("http://www.marketwatch.com/rss/newsfinder/AllMarketWatchNews/?p=type&pv=Stocks%20to%20Watch&t=Stocks%20to%20Watch&dist=sr_rss")
xmltop
xmltop
ProcessTop("marketwatch.com", xmltop, "//rss/channel/item", "pubDate", "title", "description", "link", FALSE)
nodes <- getNodeSet(xmltop, xpath)
xpath <- "//rss/channel/item"
nodes <- getNodeSet(xmltop, xpath)
nodes
nodeData <- lapply(nodes, GetNodeDetails, timeStampTag, titleTag, descTag, linkTag, getTicker)
returnData <- as.data.frame(do.call(rbind, nodeData), stringsAsFactors = FAlSE)
lapply(nodeData, names)
timeStampTag <- "pubDate"
titleTag <- "title"
descTag <- "description"
linkTag <- "link"
getTicker <- FALSE
nodes <- getNodeSet(xmltop, xpath)
nodeData <- lapply(nodes, GetNodeDetails, timeStampTag, titleTag, descTag, linkTag, getTicker)
returnData <- as.data.frame(do.call(rbind, nodeData), stringsAsFactors = FAlSE)
lapply(nodeData, names)
node <- nodes[[2]]
nData <- xmlSApply(node, function(x) xmlSApply(x, xmlValue))
names(nData) <- gsub(".text", "", names(nData))
nData <- unlist(nData)
nData
nodes[[2]]
nData <- xmlSApply(node, function(x) xmlSApply(x, xmlValue))
nData
nData <- unlist(nData)
nData
names(nData) <- gsub(".tex", "", names(nData))
nData
# Function to extract data from node
GetNodeDetails <- function(node, timeStampTag, titleTag, descTag, linkTag, getTicker) {
# Process data
nData <- xmlSApply(node, function(x) xmlSApply(x, xmlValue))
names(nData) <- gsub(".text", "", names(nData))
nData <- unlist(nData)
names(nData) <- gsub(".tex", "", names(nData))
# Get elements
timeStamp <- nData[timeStampTag]
title <- nData[titleTag]
if(!is.na(descTag)) { description <- nData[descTag] } else { description <- NA }
if(!is.na(linkTag)) { link <- nData[linkTag] } else { link <- NA }
# Tickers
if(getTicker) {
tData <- as.data.frame(nData, stringsAsFactors=FALSE)
tickers <- grep("^([A-Z]{1,5})$", tData[,1], value=TRUE)
tickers <- paste(unique(tickers),collapse=",")
} else {
tickers <- NA
}
# Return data
return(data.frame(timeStamp, title, description, link, tickers, row.names = NULL))
}
# Function to pull data and format it
ProcessTop <- function(siteName, xmltop, xpath, timeStampTag, titleTag, descTag, linkTag, getTicker) {
nodes <- getNodeSet(xmltop, xpath)
nodeData <- lapply(nodes, GetNodeDetails, timeStampTag, titleTag, descTag, linkTag, getTicker)
returnData <- as.data.frame(do.call(rbind, nodeData), stringsAsFactors = FAlSE)
returnData <- cbind(siteName, returnData)
return(returnData)
}
xmltop <- GetTop("http://www.marketwatch.com/rss/newsfinder/AllMarketWatchNews/?p=type&pv=Stocks%20to%20Watch&t=Stocks%20to%20Watch&dist=sr_rss")
newsData <- ProcessTop("marketwatch.com", xmltop, "//rss/channel/item", "pubDate", "title", "description", "link", FALSE)
xpath <- "//rss/channel/item"
newsData
xmltop <- GetTop("http://www.marketwatch.com/rss/newsfinder/AllMarketWatchNews/?p=type&pv=Stocks%20to%20Watch&t=Stocks%20to%20Watch&dist=sr_rss")
# Sub process
xpath <- "//rss/channel/item"
timeStampTag <- "pubDate"
titleTag <- "title"
descTag <- "description"
linkTag <- "link"
getTicker <- FALSE
nodes <- getNodeSet(xmltop, xpath)
nodeData <- lapply(nodes, GetNodeDetails, timeStampTag, titleTag, descTag, linkTag, getTicker)
returnData <- as.data.frame(do.call(rbind, nodeData), stringsAsFactors = FAlSE)
lapply(nodeData, names)
node <- nodes[[2]]
nData <- xmlSApply(node, function(x) xmlSApply(x, xmlValue))
names(nData) <- gsub(".text", "", names(nData))
nData <- unlist(nData)
names(nData) <- gsub(".tex", "", names(nData))
nData
# Get elements
timeStamp <- nData[timeStampTag]
title <- nData[titleTag]
if(!is.na(descTag)) { description <- nData[descTag] } else { description <- NA }
if(!is.na(linkTag)) { link <- nData[linkTag] } else { link <- NA }
# Tickers
if(getTicker) {
tData <- as.data.frame(nData, stringsAsFactors=FALSE)
tickers <- grep("^([A-Z]{1,5})$", tData[,1], value=TRUE)
tickers <- paste(unique(tickers),collapse=",")
} else {
tickers <- NA
}
data.frame(timeStamp, title, description, link, tickers, row.names = NULL)
nData[titleTag]
nData
# Function to extract data from node
GetNodeDetails <- function(node, timeStampTag, titleTag, descTag, linkTag, getTicker) {
# Process data
nData <- xmlSApply(node, function(x) xmlSApply(x, xmlValue))
names(nData) <- gsub(".text", "", names(nData))
nData <- unlist(nData)
names(nData) <- gsub(".tex", "", names(nData))
# Get elements
timeStamp <- nData[timeStampTag]
title <- nData[titleTag]
if(!is.na(descTag)) { description <- nData[descTag] } else { description <- NA }
if(!is.na(linkTag)) { link <- nData[linkTag] } else { link <- NA }
# Tickers
if(getTicker) {
tData <- as.data.frame(nData, stringsAsFactors=FALSE)
tickers <- grep("^([A-Z]{1,5})$", tData[,1], value=TRUE)
tickers <- paste(unique(tickers),collapse=",")
} else {
tickers <- NA
}
# Return data
return(data.frame(timeStamp, title, description, link, tickers, row.names = NULL))
}
xmltop <- GetTop("http://www.marketwatch.com/rss/newsfinder/AllMarketWatchNews/?p=type&pv=Stocks%20to%20Watch&t=Stocks%20to%20Watch&dist=sr_rss")
newsData <- ProcessTop("marketwatch.com", xmltop, "//rss/channel/item", "pubDate", "title", "description", "link", FALSE)
newsData
# Sub process
xpath <- "//rss/channel/item"
timeStampTag <- "pubDate"
titleTag <- "title"
descTag <- "description"
linkTag <- "link"
getTicker <- FALSE
nodes <- getNodeSet(xmltop, xpath)
nodeData <- lapply(nodes, GetNodeDetails, timeStampTag, titleTag, descTag, linkTag, getTicker)
returnData <- as.data.frame(do.call(rbind, nodeData), stringsAsFactors = FAlSE)
lapply(nodeData, names)
node <- nodes[[2]]
nData <- xmlSApply(node, function(x) xmlSApply(x, xmlValue))
nData <- unlist(nData)
names(nData) <- gsub(".text", "", names(nData))
nData
nData[timeStampTag]
nData[titleTag]
ProcessTop("marketwatch.com", xmltop, "//rss/channel/item", "pubDate", "title", "description", "link", FALSE)
# Get elements
timeStamp <- nData[timeStampTag]
title <- nData[titleTag]
if(!is.na(descTag)) { description <- nData[descTag] } else { description <- NA }
if(!is.na(linkTag)) { link <- nData[linkTag] } else { link <- NA }
# Tickers
if(getTicker) {
tData <- as.data.frame(nData, stringsAsFactors=FALSE)
tickers <- grep("^([A-Z]{1,5})$", tData[,1], value=TRUE)
tickers <- paste(unique(tickers),collapse=",")
} else {
tickers <- NA
}
data.frame(timeStamp, title, description, link, tickers, row.names = NULL)
xmltop <- GetTop("http://www.marketwatch.com/rss/newsfinder/AllMarketWatchNews/?p=type&pv=Stocks%20to%20Watch&t=Stocks%20to%20Watch&dist=sr_rss")
newsData <- ProcessTop("marketwatch.com", xmltop, "//rss/channel/item", "pubDate", "title", "description", "link", FALSE)
newsData
# Sub process
xpath <- "//rss/channel/item"
timeStampTag <- "pubDate"
titleTag <- "title"
descTag <- "description"
linkTag <- "link"
getTicker <- FALSE
nodes <- getNodeSet(xmltop, xpath)
nodeData <- lapply(nodes, GetNodeDetails, timeStampTag, titleTag, descTag, linkTag, getTicker)
returnData <- as.data.frame(do.call(rbind, nodeData), stringsAsFactors = FAlSE)
lapply(nodeData, names)
node <- nodes[[2]]
nData <- xmlSApply(node, function(x) xmlSApply(x, xmlValue))
nData <- unlist(nData)
names(nData) <- gsub(".text", "", names(nData))
nData
# Get elements
timeStamp <- nData[timeStampTag]
title <- nData[titleTag]
if(!is.na(descTag)) { description <- nData[descTag] } else { description <- NA }
if(!is.na(linkTag)) { link <- nData[linkTag] } else { link <- NA }
# Tickers
if(getTicker) {
tData <- as.data.frame(nData, stringsAsFactors=FALSE)
tickers <- grep("^([A-Z]{1,5})$", tData[,1], value=TRUE)
tickers <- paste(unique(tickers),collapse=",")
} else {
tickers <- NA
}
data.frame(timeStamp, title, description, link, tickers, row.names = NULL)
# Function to extract data from node
GetNodeDetails <- function(node, timeStampTag, titleTag, descTag, linkTag, getTicker) {
# Process data
nData <- xmlSApply(node, function(x) xmlSApply(x, xmlValue))
nData <- unlist(nData)
names(nData) <- gsub(".text", "", names(nData))
# Get elements
timeStamp <- nData[timeStampTag]
title <- nData[titleTag]
if(!is.na(descTag)) { description <- nData[descTag] } else { description <- NA }
if(!is.na(linkTag)) { link <- nData[linkTag] } else { link <- NA }
# Tickers
if(getTicker) {
tData <- as.data.frame(nData, stringsAsFactors=FALSE)
tickers <- grep("^([A-Z]{1,5})$", tData[,1], value=TRUE)
tickers <- paste(unique(tickers),collapse=",")
} else {
tickers <- NA
}
# Return data
return(data.frame(timeStamp, title, description, link, tickers, row.names = NULL))
}
xmltop <- GetTop("http://www.marketwatch.com/rss/newsfinder/AllMarketWatchNews/?p=type&pv=Stocks%20to%20Watch&t=Stocks%20to%20Watch&dist=sr_rss")
newsData <- ProcessTop("marketwatch.com", xmltop, "//rss/channel/item", "pubDate", "title", "description", "link", FALSE)
newsData
GetTop("http://feeds.thestreet.com/tsc/feeds/rss/todays-market")
xmltop <- GetTop("http://www.apple.com/pr/feeds/pr.rss")
xmltop
ProcessTop("marketwatch.com", xmltop, "//rss/channel/item", "pubDate", "title", "description", "link", FALSE)
# Create space to save data
aData <- c()
# SEEKING ALPHA
xmltop <- GetTop("http://seekingalpha.com/feed.xml")
newsData <- ProcessTop("seekingalpha.com", xmltop, "//channel/item", "pubDate", "title", NA, "link", TRUE)
aData <- rbind(aData, newsData)
# Motley Fool
xmltop <- GetTop("http://www.fool.com/feeds/index.aspx?id=foolwatch&format=rss2")
newsData <- ProcessTop("fool.com", xmltop, "//channel/item", "pubDate", "title", "description", "guid", TRUE)
aData <- rbind(aData, newsData)
# Wall Street Journal
xmltop <- GetTop("http://www.wsj.com/xml/rss/3_7031.xml")
newsData <- ProcessTop("wsj.com", xmltop, "//rss/channel/item", "pubDate", "title", "description", "link", FALSE)
aData <- rbind(aData, newsData)
# Financial Times
xmltop <- GetTop("http://www.ft.com/rss/markets")
newsData <- ProcessTop("ft.com", xmltop, "//rss/channel/item", "pubDate", "title", "description", "guid", FALSE)
aData <- rbind(aData, newsData)
# Forbes
xmltop <- GetTop("http://www.forbes.com/markets/index.xml")
newsData <- ProcessTop("forbes.com", xmltop, "//rss/channel/item", "pubDate", "title", "description", "link", FALSE)
aData <- rbind(aData, newsData)
# Market Watch
xmltop <- GetTop("http://www.marketwatch.com/rss/newsfinder/AllMarketWatchNews/?p=type&pv=Stocks%20to%20Watch&t=Stocks%20to%20Watch&dist=sr_rss")
newsData <- ProcessTop("marketwatch.com", xmltop, "//rss/channel/item", "pubDate", "title", "description", "link", FALSE)
aData <- rbind(aData, newsData)
# Apple.com
xmltop <- GetTop("http://www.apple.com/pr/feeds/pr.rss")
newsData <- ProcessTop("apple.com", xmltop, "//rss/channel/item", "pubDate", "title", "description", "link", FALSE)
aData <- rbind(aData, newsData)
# Show data collected
head(aData); dim(aData)
